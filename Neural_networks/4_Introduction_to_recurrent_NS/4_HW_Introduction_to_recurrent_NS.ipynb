{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52631f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nНейронная часть речевой маркировки \\n\\nТеперь мы будем решать проблему пометки POS с помощью нейронных сетей.\\nС точки зрения глубокого обучения, это задача прогнозирования последовательности выходных данных, \\nсогласованной с последовательностью входных данных. \\nЕсть несколько проблем, которые соответствуют этой формулировке:\\n\\n- Пометка части речи - вспомогательная задача для многих проблем НЛП\\n- Распознавание именованных сущностей - для чат-ботов и веб-сканеров\\n- Прогнозирование структуры белка - для биоинформатики\\n\\nУдалите CWD из sys.path, пока мы загружаем материалы.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Нейронная часть речевой маркировки \n",
    "\n",
    "Теперь мы будем решать проблему пометки POS с помощью нейронных сетей.\n",
    "С точки зрения глубокого обучения, это задача прогнозирования последовательности выходных данных, \n",
    "согласованной с последовательностью входных данных. \n",
    "Есть несколько проблем, которые соответствуют этой формулировке:\n",
    "\n",
    "- Пометка части речи - вспомогательная задача для многих проблем НЛП\n",
    "- Распознавание именованных сущностей - для чат-ботов и веб-сканеров\n",
    "- Прогнозирование структуры белка - для биоинформатики\n",
    "\n",
    "Удалите CWD из sys.path, пока мы загружаем материалы.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf6b7127",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\zlatt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\zlatt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "C:\\Users\\zlatt\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 'DET'),\n",
       " ('fulton', 'NOUN'),\n",
       " ('county', 'NOUN'),\n",
       " ('grand', 'ADJ'),\n",
       " ('jury', 'NOUN'),\n",
       " ('said', 'VERB'),\n",
       " ('friday', 'NOUN'),\n",
       " ('an', 'DET'),\n",
       " ('investigation', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " (\"atlanta's\", 'NOUN'),\n",
       " ('recent', 'ADJ'),\n",
       " ('primary', 'NOUN'),\n",
       " ('election', 'NOUN'),\n",
       " ('produced', 'VERB'),\n",
       " ('``', '.'),\n",
       " ('no', 'DET'),\n",
       " ('evidence', 'NOUN'),\n",
       " (\"''\", '.'),\n",
       " ('that', 'ADP'),\n",
       " ('any', 'DET'),\n",
       " ('irregularities', 'NOUN'),\n",
       " ('took', 'VERB'),\n",
       " ('place', 'NOUN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter   # Подсчет количества повторений элементов в последовательности\n",
    "from collections import defaultdict # defaultdict автоматически назначает ноль как значение любому ключу, который еще не имеет значения.\n",
    "import keras\n",
    "import keras.layers as L\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from tensorflow_addons.layers import CRF\n",
    "\n",
    "\n",
    "# Процесс классификации слов по частям речи и их соответствующей маркировки известен как тегирование части речи , \n",
    "# POS-тегирование или просто тегирование . Части речи также известны как классы слов или лексические категории . \n",
    "# Набор тегов, используемых для конкретной задачи, называется набором тегов\n",
    "# Тегер части речи или POS-tagger обрабатывает последовательность слов и присоединяет часть тега речи к каждому слову\n",
    "\n",
    "# По соглашению в NLTK маркированный токен представляется с помощью кортежа, состоящего из токена и тега. \n",
    "# Мы можем создать один из этих специальных кортежей из \n",
    "# стандартного строкового представления помеченного токена, используя функцию str2tuple ()\n",
    "\n",
    "# Некоторые из корпусов, включенных в NLTK, были помечены как часть речи. Например, Brown Corpus\n",
    "# теги части речи были преобразованы в верхний регистр, \n",
    "# поскольку это стало стандартной практикой для Brown Corpus\n",
    "# Когда корпус содержит текст с тегами, интерфейс корпуса NLTK будет иметь метод tagged_words ()\n",
    "\n",
    "# Метод tagged_sents () делит помеченные слова на предложения, \n",
    "# а не представляет их как один большой список. Это будет полезно, \n",
    "# когда мы перейдем к разработке автоматических тегеров, \n",
    "# поскольку они обучаются и тестируются на списках предложений, а не слов\n",
    "# Взято отсюда: категоризация и теги слов - https://www.nltk.org/book/ch05.html\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "data = nltk.corpus.brown.tagged_sents(tagset='universal') # универсальный набор тегов, также могут быть категории, пр, новостей\n",
    "all_tags = ['#EOS#','#UNK#','ADV', 'NOUN', 'ADP', 'PRON', 'DET', '.', 'PRT', 'VERB', 'X', 'NUM', 'CONJ', 'ADJ']\n",
    "\n",
    "data = np.array([ [(word.lower(),tag) for word,tag in sentence] for sentence in data ])\n",
    "data[0] # первый из списков (первое предложение)\n",
    "# data\n",
    "# dtype==data\n",
    "# Предупреждение:\n",
    "# Создание ndarray из неровных вложенных последовательностей \n",
    "# (которые представляют собой список или кортеж списков или кортежей или ndarray разной длины или формы) \n",
    "# не рекомендуется. \n",
    "# Если вы намеревались это сделать, вы должны указать \"dtype=объект\" при создании ndarray\n",
    "\n",
    "# Работа с корпусами: https://www.nltk.org/api/nltk.corpus.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9d02e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b8746d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>NOUN</td><td>ADP</td><td>NOUN</td><td>NOUN</td><td>NOUN</td><td>NOUN</td><td>VERB</td><td>ADV</td><td>VERB</td><td>ADP</td><td>DET</td><td>ADJ</td><td>NOUN</td><td>.</td></tr><td>implementation</td><td>of</td><td>georgia's</td><td>automobile</td><td>title</td><td>law</td><td>was</td><td>also</td><td>recommended</td><td>by</td><td>the</td><td>outgoing</td><td>jury</td><td>.</td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>PRON</td><td>VERB</td><td>ADP</td><td>DET</td><td>NOUN</td><td>.</td><td>VERB</td><td>NOUN</td><td>PRT</td><td>VERB</td><td>.</td><td>DET</td><td>NOUN</td><td>.</td></tr><td>it</td><td>urged</td><td>that</td><td>the</td><td>city</td><td>``</td><td>take</td><td>steps</td><td>to</td><td>remedy</td><td>''</td><td>this</td><td>problem</td><td>.</td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>NOUN</td><td>VERB</td></tr><td>merger</td><td>proposed</td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Чтобы добавить изображения в нашу программу, можно использовать библиотеку IPython. \n",
    "# Можно генерировать HTML напрямую, используя все его возможности\n",
    "# документация https://ipython.org/ipython-doc/3/api/generated/IPython.display.html\n",
    "from IPython.display import HTML, display   # display - объект, в котором хранятся данные для отображения\n",
    "def draw(sentence):\n",
    "    words,tags = zip(*sentence)                                          # zip() в Python определяется как zip(* iterables). \n",
    "#                                                                         Функция принимает итераторы (то есть последователь-\n",
    "#                                                                         ности) в качестве аргументов \n",
    "#                                                                         и возвращает то же итератор. Этот итератор генерирует \n",
    "#                                                                         серию кортежей, содержащих элементы из каждой итерации.\n",
    "#                                                                         zip() может принимать любые типы итераций, такие как \n",
    "#                                                                         файлы, списки, кортежи, словари, наборы и т. д.\n",
    "#                                                                         numbers = [1, 2, 3]\n",
    "#                                                                         numbers = [1, 2, 3]\n",
    "#                                                                         letters = ['a', 'b', 'c']\n",
    "#                                                                         zipped = zip(numbers, letters)\n",
    "#                                                                         list(zipped)\n",
    "#                                                                           # [(1, 'a'), (2, 'b'), (3, 'c')]\n",
    "#                                                                          \n",
    "    display(HTML('<table><tr>{tags}</tr>{words}<tr></table>'.format(\n",
    "        words = '<td>{}</td>'.format('</td><td>'.join(words)),\n",
    "                tags = '<td>{}</td>'.format('</td><td>'.join(tags)))))\n",
    "    \n",
    "    \n",
    "draw(data[11])\n",
    "draw(data[10])\n",
    "draw(data[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b73ecd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage = 0.92876\n"
     ]
    }
   ],
   "source": [
    "# Создание словарей\n",
    "# Как и раньше, мы должны построить сопоставление токенов с целочисленными идентификаторами. \n",
    "# На этот раз наша модель работает на уровне слов, обрабатывая одно слово за шаг RNN (рекурентной сети). \n",
    "# Это означает, что нам придется иметь дело с гораздо большим словарным запасом.\n",
    "\n",
    "# К счастью для нас, мы получаем эти слова только в качестве входных данных, \n",
    "# т. е. нам не нужно их предсказывать. Это означает, что мы можем иметь \n",
    "# большой словарный запас бесплатно, используя встраивания слов.\n",
    "\n",
    "\n",
    "word_counts = Counter()\n",
    "for sentence in data:\n",
    "    words,tags = zip(*sentence)\n",
    "    word_counts.update(words)\n",
    "\n",
    "all_words = ['#EOS#','#UNK#'] + list(list(zip(*word_counts.most_common(10000)))[0]) # most_common([n])- \"самый распространенный\"\n",
    "                                                                                    # возвращает список из n наиболее \n",
    "                                                                                    # распространенных элементов и их кол-во \n",
    "                                                                                    # от наиболее распространенных до наименее. \n",
    "                                                                                     # Если n опущено или None, \n",
    "                                                                                    # метод cnt.most_common() \n",
    "                                                                                    # возвращает все элементы в счетчике \n",
    "               # Подробнее: https://docs-python.ru/standart-library/modul-collections-python/klass-counter-modulja-collections/\n",
    "# Измерим, какая доля слов данных содержится в словаре\n",
    "print(\"Coverage = %.5f\" % (float(sum(word_counts[w] for w in all_words)) / sum(word_counts.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dd2707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defaultdict автоматически назначает ноль как значение любому ключу, который еще не имеет значения.\n",
    "word_to_id = defaultdict(lambda:1, { word: i for i, word in enumerate(all_words) })\n",
    "tag_to_id = { tag: i for i, tag in enumerate(all_tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8070369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#EOS#', 0),\n",
       " ('#UNK#', 1),\n",
       " ('the', 2),\n",
       " (',', 3),\n",
       " ('.', 4),\n",
       " ('of', 5),\n",
       " ('and', 6),\n",
       " ('to', 7),\n",
       " ('a', 8),\n",
       " ('in', 9)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk = list(word_to_id.items())\n",
    "tk[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bad4b1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#EOS#', 0),\n",
       " ('#UNK#', 1),\n",
       " ('ADV', 2),\n",
       " ('NOUN', 3),\n",
       " ('ADP', 4),\n",
       " ('PRON', 5),\n",
       " ('DET', 6),\n",
       " ('.', 7),\n",
       " ('PRT', 8),\n",
       " ('VERB', 9)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = list(tag_to_id.items())\n",
    "tr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "317ffac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# преобразуем слова и теги в матрицу фиксированного размера\n",
    "def to_matrix(lines, token_to_id, max_len=None, pad=0, dtype='int32', time_major=False):\n",
    "    \"\"\"Преобразует список имен в rnn-усваиваемую матрицу с дополнениями, добавленными после конца (end)\"\"\"\n",
    "#     max(map(len,lines)) вернет длину максимально длинной строки\n",
    "    max_len = max_len or max(map(len,lines))                      # позволяет обрабатывать и преобразовывать все элементы в \n",
    "                                                                  # итерируемом объекте без использования явного цикла for, \n",
    "                                                                  # метода, широко известного как сопоставление (mapping). \n",
    "                                                                  # map() полезен, \n",
    "                                                                  # когда нужно применить функцию преобразования \n",
    "                                                                  # к каждому элементу в коллекции или в массиве \n",
    "                                                                  # и преобразовать их в новый массив\n",
    "                                                                    # def square(number):\n",
    "                                                                    #   return number ** 2\n",
    "                                                                    # numbers = [1, 2, 3, 4, 5]\n",
    "                                                                    # squared = map(square, numbers)\n",
    "                                                                    # или squared = map(lambda num: num ** 2, numbers)\n",
    "                                                                    # list(squared)\n",
    "                                                                    #    [1, 4, 9, 16, 25]\n",
    "#   empty() возвращает новый массив заданной формы и типа без инициированных записей:\n",
    "#   здесь мы фиксируем размер будущей матрицы через len(lines) и max_len\n",
    "    matrix = np.empty([len(lines), max_len],dtype)\n",
    "    matrix.fill(pad)\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        # Определяет поведение при доступе к элементу, \n",
    "        # используя синтаксис self[key]. \n",
    "        # То же относится и к протоколу изменяемых и к протоколу неизменяемых контейнеров\n",
    "        # про магические методы https://habr.com/ru/post/186608/ \n",
    "        line_ix = list(map(token_to_id.__getitem__,lines[i]))[:max_len]\n",
    "        matrix[i,:len(line_ix)] = line_ix\n",
    "\n",
    "    return matrix.T if time_major else matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c91627e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word ids:\n",
      "[[   2 3057    5    2 2238 1334 4238 2454    3    6   19   26 1070   69\n",
      "     8 2088    6    3    1    3  266   65  342    2    1    3    2  315\n",
      "     1    9   87  216 3322   69 1558    4    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  45   12    8  511 8419    6   60 3246   39    2    1    1    3    2\n",
      "   845    1    3    1    3   10 9910    2    1 3470    9   43    1    1\n",
      "     3    6    2 1046  385   73 4562    3    9    2    1    1 3250    3\n",
      "    12   10    2  861 5240   12    8 8936  121    1    4]\n",
      " [  33   64   26   12  445    7 7346    9    8 3337    3    1 2811    3\n",
      "     2  463  572    2    1    1 1649   12    1    4    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]]\n",
      "Tag ids:\n",
      "[[ 6  3  4  6  3  3  9  9  7 12  4  5  9  4  6  3 12  7  9  7  9  8  4  6\n",
      "   3  7  6 13  3  4  6  3  9  4  3  7  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0]\n",
      " [ 5  9  6  9  3 12  6  3  7  6 13  3  7  6 13  3  7 13  7  5  9  6  3  3\n",
      "   4  6 13  3  7 12  6  3  6 13  3  7  4  6  3  9  3  7  9  4  6 13  3  9\n",
      "   6  3  2 13  7]\n",
      " [ 4  6  5  9 13  4  3  4  6 13  7 13  3  7  6  3  4  6 13  3  3  9  9  7\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "batch_words, batch_tags = zip(*[zip(*sentence) for sentence in data[-3:]])\n",
    "\n",
    "print(\"Word ids:\")\n",
    "print(to_matrix(batch_words, word_to_id))\n",
    "print(\"Tag ids:\")\n",
    "print(to_matrix(batch_tags, tag_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ea78e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построить модель\n",
    "# В отличие от нашей предыдущей работы, \n",
    "# на этот раз мы сосредоточимся на высокоуровневом интерфейсе keras для рекуррентных нейронных сетей. \n",
    "# Это так просто, как это возможно при помощи RNN, \n",
    "# все это несколько ограничивает для сложных задач, таких как seq2seq.\n",
    "\n",
    "# По умолчанию все RNN keras применяются ко всей последовательности входных данных \n",
    "# и создают последовательность скрытых состояний \n",
    "# (return_sequences=True) или только последнее скрытое состояние \n",
    "#  (return_sequences=False). Все повторения происходят под капотом.\n",
    "\n",
    "# В верхней части нашей модели нам нужно наносить \n",
    "#  Dense layer (плотный слой) на каждый временной шаг независимо. \n",
    "#  На данный момент по умолчанию keras.layers.Dense будет применяться один раз ко всем объединенным временным шагам. \n",
    "#  Для модификации Dense layer мы используем keras.layers.TimeDistributed,\n",
    "#  чтобы он применялся как по пакетной (batch), так и по временной осям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ef69197",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(L.InputLayer([None],dtype='int32'))\n",
    "# Embedding - кодирование слов\n",
    "# модель принимает на вход матрицу целых чисел размерностью len(all_words) строк на 50 столбцов \n",
    "model.add(L.Embedding(len(all_words),50))    \n",
    "# SimpleRNN() - полносвязная RNN в которой выход предыдущего временного шага \n",
    "# должен быть передан в следующий шаг\n",
    "# Слой RNN может также возвращать всю \n",
    "# последовательность выходных данных для каждого элемента \n",
    "# (по одному вектору на каждый шаг), если указываем return_sequences=True\n",
    "model.add(L.SimpleRNN(64,return_sequences=True))\n",
    "\n",
    "#добавим верхний слой, который предсказывает вероятности тегов\n",
    "stepwise_dense = L.Dense(len(all_tags),activation='softmax')\n",
    "# L.TimeDistributed() - эта оболочка позволяет применять слой к каждому временному фрагменту ввода\n",
    "# Каждый вход должен быть как минимум трехмерным, \n",
    "# и размер первого индекса первого входа будет считаться временным измерением.\n",
    "stepwise_dense = L.TimeDistributed(stepwise_dense)\n",
    "model.add(stepwise_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08d32f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение: в этом случае мы не хотим заранее \n",
    "# готовить весь набор данных для обучения. \n",
    "# Основная причина заключается в том, \n",
    "# что длина каждого пакета зависит \n",
    "# от максимальной длины предложения в пакете. \n",
    "# Это оставляет нам два варианта: \n",
    "# использовать пользовательский обучающий код, \n",
    "# как на предыдущем семинаре, или использовать генераторы.\n",
    "\n",
    "# В моделях Keras есть метод model.fit_generator, \n",
    "# который принимает генератор python, \n",
    "# выдающий по одной партии за раз. \n",
    "# Но сначала нам нужно реализовать такой генератор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b77c91ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "def generate_batches(sentences,batch_size=BATCH_SIZE,max_len=None,pad=0):\n",
    "    assert isinstance(sentences,np.ndarray),'Убедитесь, что предложения - это массив numpy array'\n",
    "    \n",
    "    while True:\n",
    "        # random.permutation() возвращает случайную перестановку (permutation)\n",
    "        # элементов массива или случайную последовательность заданной длинны из его элементов.\n",
    "        # Данная функция выполняет перестановку только по первой оси, \n",
    "        # поэтому для многомерных массивов возвращается перестановка его подмассивов,\n",
    "        # в то время как содержание этих подмассивов не перемешивается\n",
    "        indices = np.random.permutation(np.arange(len(sentences)))\n",
    "        for start in range(0,len(indices)-1,batch_size):\n",
    "            batch_indices = indices[start:start+batch_size]\n",
    "            batch_words,batch_tags = [],[]\n",
    "            for sent in sentences[batch_indices]:\n",
    "                words,tags = zip(*sent)\n",
    "                batch_words.append(words)\n",
    "                batch_tags.append(tags)\n",
    "\n",
    "            batch_words = to_matrix(batch_words,word_to_id,max_len,pad)\n",
    "            batch_tags = to_matrix(batch_tags,tag_to_id,max_len,pad)\n",
    "\n",
    "            batch_tags_1hot = to_categorical(batch_tags,len(all_tags)).reshape(batch_tags.shape+(-1,))\n",
    "            yield batch_words,batch_tags_1hot\n",
    "            # Yield — это ключевое слово в Python, которое используется для возврата из функции \n",
    "            # с сохранением состояния ее локальных переменных, \n",
    "            # и при повторном вызове такой функции выполнение продолжается с оператора yield, \n",
    "            # на котором ее работа была прервана"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea364495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обратные вызовы: Еще одна вещь, которая нам нужна, - \n",
    "# это измерение производительности модели. \n",
    "# Сложность заключается в том, чтобы не подсчитывать точность \n",
    "# после окончания предложения (при заполнении) и убедиться, \n",
    "# что мы подсчитываем все данные проверки ровно один раз.\n",
    "\n",
    "# Хотя нет ничего невозможного в том, \n",
    "# чтобы убедить Keras сделать все это, \n",
    "# мы можем также написать наш собственный колбэк, \n",
    "# который сделает это. Колбэки Keras \n",
    "# позволяют нам писать пользовательский код, \n",
    "# который будет выполняться один раз в каждую эпоху \n",
    "# или каждый мини-batch. Мы определим его с помощью LambdaCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e1ae03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test_accuracy(model):\n",
    "    test_words,test_tags = zip(*[zip(*sentence) for sentence in test_data])\n",
    "    test_words,test_tags = to_matrix(test_words,word_to_id),to_matrix(test_tags,tag_to_id)\n",
    "\n",
    "    #предскажем вероятности тегов размера [batch,time,n_tags]\n",
    "    predicted_tag_probabilities = model.predict(test_words,verbose=1)\n",
    "    # argmax() возвращает индекс максимального значения вдоль указанной оси\n",
    "    predicted_tags = predicted_tag_probabilities.argmax(axis=-1)\n",
    "\n",
    "    #вычислим точность без учета заполнения\n",
    "    # (np.logical_and) вычисляет значение истинности x1 и x2 поэлементно.\n",
    "    # x = np.arange(5)\n",
    "    # np.logical_and(x>1, x<4)\n",
    "       # array([False, False,  True,  True, False])\n",
    "    # в numerator должны получить пересечение истинности по двум позициям:\n",
    "    # предсказанные теги равны тестовым и и тестовые слова этих тегов не равны нулю\n",
    "    numerator = np.sum(np.logical_and((predicted_tags == test_tags),(test_words != 0)))\n",
    "    denominator = np.sum(test_words != 0)\n",
    "    return float(numerator)/denominator\n",
    "\n",
    "\n",
    "class EvaluateAccuracy(keras.callbacks.Callback):\n",
    "    \"\"\"Оценка точности\"\"\"\n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        # Стандартный вывод Python является буферизированным (это означает, \n",
    "        # что он собирает некоторые данные, «записанные» в стандарте, \n",
    "        # прежде чем записать их в терминал). Вызов sys.stdout.flush()заставляет его «очистить» буфер, \n",
    "        # что означает, что он будет записывать все, \n",
    "        # что есть в буфере, на терминал, даже если обычно он будет ждать перед этим.\n",
    "        # подробнее о буферах данных - https://en.wikipedia.org/wiki/Data_buffer \n",
    "        sys.stdout.flush()\n",
    "        print(\"\\nMeasuring validation accuracy...\")\n",
    "        acc = compute_test_accuracy(self.model)\n",
    "        print(\"\\nValidation accuracy: %.5f\\n\"%acc)\n",
    "        sys.stdout.flush()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ed2f753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zlatt\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1343/1343 [==============================] - 19s 14ms/step - loss: 0.2452\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 5s 10ms/step\n",
      "\n",
      "Validation accuracy: 0.94050\n",
      "\n",
      "Epoch 2/5\n",
      "1343/1343 [==============================] - 18s 13ms/step - loss: 0.0577\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 4s 9ms/step\n",
      "\n",
      "Validation accuracy: 0.94522\n",
      "\n",
      "Epoch 3/5\n",
      "1343/1343 [==============================] - 19s 14ms/step - loss: 0.0505\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 5s 11ms/step\n",
      "\n",
      "Validation accuracy: 0.94617\n",
      "\n",
      "Epoch 4/5\n",
      "1343/1343 [==============================] - 22s 16ms/step - loss: 0.0460\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 5s 11ms/step\n",
      "\n",
      "Validation accuracy: 0.94640\n",
      "\n",
      "Epoch 5/5\n",
      "1343/1343 [==============================] - 22s 16ms/step - loss: 0.0418\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 5s 11ms/step\n",
      "\n",
      "Validation accuracy: 0.94553\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x219298a7808>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','categorical_crossentropy')\n",
    "\n",
    "model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
    "                    callbacks=[EvaluateAccuracy()], epochs=5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "805f1e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 5s 11ms/step\n",
      "Final accuracy: 0.94553\n"
     ]
    }
   ],
   "source": [
    "# Измерим конечную точность на всем наборе тестов\n",
    "acc = compute_test_accuracy(model)\n",
    "print(\"Final accuracy: %.5f\"%acc)\n",
    "\n",
    "assert acc>0.94, \"Keras снова взбесился, пожалуйста, свяжитесь с персоналом курса.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bf316f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Движение в двух направлениях\n",
    "# Поскольку мы анализируем полную последовательность, для нас законно изучать будущие данные.\n",
    "\n",
    "# Простой способ добиться этого-идти в обоих направлениях одновременно, \n",
    "# создавая двунаправленный RNN (bidirectional RNN).\n",
    "# В Keras мы можем добиться этого как вручную (с помощью двух LSTM и объединения), так и с помощью keras.layers.Bidirectional.\n",
    "\n",
    "# Этот работает так же, как и распределенный по времени, который мы видели ранее: \n",
    "# вы обертываете его вокруг повторяющегося слоя (сейчас SimpleRNN, а позже LSTM/GRU), \n",
    "# и он фактически создает два слоя под капотом.\n",
    "# Ваша первая задача-использовать такой слой нашего POS-теггера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10c8530d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 50)          500100    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 128)         14720     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 14)          1806      \n",
      "=================================================================\n",
      "Total params: 516,626\n",
      "Trainable params: 516,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1343/1343 [==============================] - 37s 26ms/step - loss: 0.2091\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 9s 20ms/step\n",
      "\n",
      "Validation accuracy: 0.95555\n",
      "\n",
      "Epoch 2/5\n",
      "1343/1343 [==============================] - 36s 26ms/step - loss: 0.0441\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 9s 19ms/step\n",
      "\n",
      "Validation accuracy: 0.96007\n",
      "\n",
      "Epoch 3/5\n",
      "1343/1343 [==============================] - 36s 27ms/step - loss: 0.0364\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 9s 20ms/step\n",
      "\n",
      "Validation accuracy: 0.96148\n",
      "\n",
      "Epoch 4/5\n",
      "1343/1343 [==============================] - 34s 25ms/step - loss: 0.0310\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 8s 18ms/step\n",
      "\n",
      "Validation accuracy: 0.96128\n",
      "\n",
      "Epoch 5/5\n",
      "1343/1343 [==============================] - 31s 23ms/step - loss: 0.0261\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 8s 17ms/step\n",
      "\n",
      "Validation accuracy: 0.96089\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2192bd93188>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Определим модель, которая использует bidirectional SimpleRNN\n",
    "# from keras.layers import SimpleRNN, Bidirectional, Embedding, LSTM, Dense\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# <Your code here!>\n",
    "\n",
    "\n",
    "model.add(L.Embedding(len(all_words),50))\n",
    "\n",
    "forward_layer = L.SimpleRNN(64, return_sequences=True) # слой можно заменить на LSTM (более продвинут)\n",
    "backward_layer = L.SimpleRNN(64, activation='relu', return_sequences=True, go_backwards=True)\n",
    "model.add(L.Bidirectional(forward_layer, backward_layer=backward_layer))\n",
    "\n",
    "stepwise_dense = L.Dense(len(all_tags),activation='softmax') # для полносвязной сети Dense здесь обязательна с функцией softmax\n",
    "stepwise_dense = L.TimeDistributed(stepwise_dense)\n",
    "model.add(stepwise_dense)\n",
    "model.summary()\n",
    "model.compile('adam','categorical_crossentropy')\n",
    "model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
    "                    callbacks=[EvaluateAccuracy()], epochs=5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "500e89c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 8s 17ms/step\n",
      "\n",
      "Final accuracy: 0.96089\n",
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "acc = compute_test_accuracy(model)\n",
    "print(\"\\nFinal accuracy: %.5f\"%acc)\n",
    "\n",
    "assert acc>0.96, \"Bidirectional RNNs are better than this!\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e452af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задача I: Структурированные функции потерь (больше бонусных баллов)\n",
    "\n",
    "# Поскольку мы помечаем всю последовательность сразу, мы могли бы также обучить нашу сеть этому. \n",
    "# Помните линейный CRF из лекции? Вы также можете использовать его \n",
    "# в качестве функции потерь для вашего RNN\n",
    "\n",
    "# Существует несколько способов сделать это, \n",
    "# но мы бы рекомендовали начать с условных случайных полей\n",
    "# Вы можете подключить CRF в качестве функции потерь и по-прежнему тренироваться с помощью backprop. \n",
    "# Для вас даже есть какая-то аккуратная реализация tensorflow.\n",
    "# Кроме того, вы можете настроить свою модель на предыдущие теги \n",
    "# (сделать ее авторегрессионной) и выполнить поиск луча по этой модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e68fd510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Несколько советов\n",
    "# Здесь есть еще несколько советов о том, как улучшить обучение, \n",
    "# которые немного сложнее реализовать. Мы настоятельно рекомендуем вам \n",
    "# попробовать их после того, как у вас будет хорошая начальная модель.\n",
    "\n",
    "# 1) Используйте предварительно подготовленные вложения (pre-trained embeddings): вы можете \n",
    "# использовать предварительно подготовленные веса оттуда, чтобы запустить свой слой встраивания.\n",
    "#   -Слой встраивания имеет матрицу W (слой.W), который содержит встраивания слов \n",
    "# для каждого слова в словаре. Вы можете просто перезаписать их с помощью tf.assign.\n",
    "#   - При использовании предварительно подготовленных вложений \n",
    "# обратите внимание на то, что словарь модели отличается от вашего собственного.\n",
    "#   - Возможно, вам захочется переключить trainable=FALSE для встраивания слоя в первые несколько эпох, \n",
    "# как при обычной тонкой настройке.\n",
    "# 2) Выходите за рамки SimpleRNN: есть keras.layers.LSTM и keras.layers.GRU\n",
    "#   - Если вы хотите использовать пользовательскую рекуррентную ячейку, прочтите это\n",
    "#   - Вы также можете использовать 1D свертки (keras.слои.Conv1D). \n",
    "# Они часто так же хороши, как и повторяющиеся слои, но с меньшей подгонкой.\n",
    "# 3) Укладывайте больше слоев: если в этом курсе есть общий мотив, то он касается укладки слоев\n",
    "#   - Вы можете просто добавить повторяющиеся и 1dconv слои друг на друга, и керас поймет это\n",
    "#   - Просто помните, что более крупным сетям может потребоваться больше эпох для обучения\n",
    "# 4) Регуляризация: вы можете применять отсевы как обычно, но также и специфичным для RNN способом\n",
    "#   - keras.layers.Dropout работает между слоями RNN\n",
    "#   - Рекуррентные слои также имеют параметр recurrent_dropout\n",
    "# 5) Обрезка градиента (Gradient clipping): если ваше обучение не так стабильно, как вам хотелось бы, \n",
    "# установите clipnorm в своем оптимизаторе.\n",
    "#   - То есть, это хорошая идея - следить за своей кривой потерь при каждом мини-матче. П\n",
    "# опробуйте tensorboard callback или что-то подобное.\n",
    "# 6) Отсев слов (Word Dropout): tl; dr случайным образом заменяет слова на UNK во время обучения.\n",
    "#   - Это также может имитировать увеличение количества неизвестных слов в наборе тестов\n",
    "# 7) Больший словарный запас: Вы можете повысить производительность, \n",
    "# увеличив входной словарь вашей модели с 5000 до каждого отдельного слова!\n",
    "#   - Просто убедитесь, что ваша модель не подходит по многим параметрам.\n",
    "#   - В сочетании с регуляризаторами или предварительно обученными векторами слов \n",
    "# это может быть действительно хорошо, потому что сейчас наша модель слепа к > 5% слов.\n",
    "# 8) Более эффективная дозировка (batching): прямо сейчас TF тратит много времени на повторение \"0\" с\n",
    "#   - Это происходит потому, что пакет всегда дополняется до длины самого длинного предложения\n",
    "#   - Вы можете ускорить процесс, предварительно сгенерировав пакеты одинаковой длины \n",
    "# и снабдив их случайно выбранным предварительно сгенерированным пакетом.\n",
    "#   - Это технически нарушает предположение о i.i.d., но это работает, \n",
    "# если вы не придумаете какие-то безумные архитектуры rnn.\n",
    "# 9) Самый важный совет: не втискивайте все сразу!\n",
    "#   - Если вы добавите много модификаций, некоторые из них почти неизбежно окажутся вредными, \n",
    "# и вы никогда не узнаете, какие из них есть.\n",
    "#   - Попробуйте вместо этого выполнить небольшие итерации и записать результаты эксперимента, \n",
    "# чтобы направлять дальнейший поиск.\n",
    "# Удачной охоты!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4799606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_viterbi_accuracy\n",
    "\n",
    "inputs = L.Input(shape=(None,))     # если делаем через Sequential, то input и output не нужны \n",
    "x = L.Embedding(len(all_words),50)(inputs)\n",
    "x = L.Bidirectional(L.SimpleRNN(64, return_sequences=True))(x)\n",
    "x = L.Bidirectional(L.SimpleRNN(32, activation='relu', return_sequences=True, go_backwards=True))(x)\n",
    "x = L.Dense(len(all_tags),activation='softmax')(x)\n",
    "crf = CRF(len(all_tags))\n",
    "outputs = crf(x)\n",
    "model = keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e14a73d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\zlatt\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:853 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\zlatt\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:842 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\zlatt\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\zlatt\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\zlatt\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\zlatt\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:835 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\zlatt\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:791 train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    C:\\Users\\zlatt\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:522 minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    C:\\Users\\zlatt\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:622 apply_gradients\n        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\n    C:\\Users\\zlatt\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\utils.py:73 filter_empty_gradients\n        ([v.name for _, v in grads_and_vars],))\n\n    ValueError: No gradients provided for any variable: ['embedding_3/embeddings:0', 'bidirectional_3/forward_simple_rnn_5/simple_rnn_cell_11/kernel:0', 'bidirectional_3/forward_simple_rnn_5/simple_rnn_cell_11/recurrent_kernel:0', 'bidirectional_3/forward_simple_rnn_5/simple_rnn_cell_11/bias:0', 'bidirectional_3/backward_simple_rnn_5/simple_rnn_cell_12/kernel:0', 'bidirectional_3/backward_simple_rnn_5/simple_rnn_cell_12/recurrent_kernel:0', 'bidirectional_3/backward_simple_rnn_5/simple_rnn_cell_12/bias:0', 'bidirectional_4/forward_simple_rnn_6/simple_rnn_cell_14/kernel:0', 'bidirectional_4/forward_simple_rnn_6/simple_rnn_cell_14/recurrent_kernel:0', 'bidirectional_4/forward_simple_rnn_6/simple_rnn_cell_14/bias:0', 'bidirectional_4/backward_simple_rnn_6/simple_rnn_cell_15/kernel:0', 'bidirectional_4/backward_simple_rnn_6/simple_rnn_cell_15/recurrent_kernel:0', 'bidirectional_4/backward_simple_rnn_6/simple_rnn_cell_15/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0', 'crf/kernel:0', 'crf/chain_kernel:0', 'crf/bias:0', 'crf/left_boundary:0', 'crf/right_boundary:0'].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-6186f77e445a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,callbacks=[EvaluateAccuracy()],\n\u001b[1;32m----> 3\u001b[1;33m                     epochs=5,)\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1987\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1988\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1989\u001b[1;33m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1990\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1991\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_not_generate_docs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    759\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 760\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3065\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3066\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3067\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3463\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3306\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3307\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3308\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3309\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3310\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1007\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1008\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    669\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    992\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    993\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 994\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    995\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\zlatt\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:853 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\zlatt\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:842 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\zlatt\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\zlatt\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\zlatt\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\zlatt\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:835 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\zlatt\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:791 train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    C:\\Users\\zlatt\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:522 minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    C:\\Users\\zlatt\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:622 apply_gradients\n        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\n    C:\\Users\\zlatt\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\utils.py:73 filter_empty_gradients\n        ([v.name for _, v in grads_and_vars],))\n\n    ValueError: No gradients provided for any variable: ['embedding_3/embeddings:0', 'bidirectional_3/forward_simple_rnn_5/simple_rnn_cell_11/kernel:0', 'bidirectional_3/forward_simple_rnn_5/simple_rnn_cell_11/recurrent_kernel:0', 'bidirectional_3/forward_simple_rnn_5/simple_rnn_cell_11/bias:0', 'bidirectional_3/backward_simple_rnn_5/simple_rnn_cell_12/kernel:0', 'bidirectional_3/backward_simple_rnn_5/simple_rnn_cell_12/recurrent_kernel:0', 'bidirectional_3/backward_simple_rnn_5/simple_rnn_cell_12/bias:0', 'bidirectional_4/forward_simple_rnn_6/simple_rnn_cell_14/kernel:0', 'bidirectional_4/forward_simple_rnn_6/simple_rnn_cell_14/recurrent_kernel:0', 'bidirectional_4/forward_simple_rnn_6/simple_rnn_cell_14/bias:0', 'bidirectional_4/backward_simple_rnn_6/simple_rnn_cell_15/kernel:0', 'bidirectional_4/backward_simple_rnn_6/simple_rnn_cell_15/recurrent_kernel:0', 'bidirectional_4/backward_simple_rnn_6/simple_rnn_cell_15/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0', 'crf/kernel:0', 'crf/chain_kernel:0', 'crf/bias:0', 'crf/left_boundary:0', 'crf/right_boundary:0'].\n"
     ]
    }
   ],
   "source": [
    "model.compile('adam', loss='categorical_crossentropy')\n",
    "model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,callbacks=[EvaluateAccuracy()],\n",
    "                    epochs=5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b2699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.test.is_gpu_available()\n",
    "# from keras_contrib.layers import CRF\n",
    "# from keras_contrib.losses import crf_loss\n",
    "# from keras_contrib.metrics import crf_viterbi_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a73c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
