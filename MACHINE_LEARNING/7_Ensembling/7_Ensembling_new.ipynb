{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Теперь решаем задачу регрессии - предскажем цены на недвижимость. \n",
    "Использовать датасет https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data (train.csv)\n",
    "2. Построить случайный лес, вывести важность признаков\n",
    "3. Обучить стекинг как минимум 3х моделей, использовать хотя бы 1 линейную модель и 1 нелинейную\n",
    "В качестве решения: Jupyter notebook с кодом, комментариями и графиками\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler # Стандартизация функций путем удаления среднего \n",
    "                                                  # и масштабирования до единичной дисперсии\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>1456</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7917</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>1457</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>85.0</td>\n",
       "      <td>13175</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>1458</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>66.0</td>\n",
       "      <td>9042</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GdPrv</td>\n",
       "      <td>Shed</td>\n",
       "      <td>2500</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>1459</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>9717</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>142125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>1460</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9937</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>147500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0        1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1        2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2        3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3        4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4        5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "...    ...         ...      ...          ...      ...    ...   ...      ...   \n",
       "1455  1456          60       RL         62.0     7917   Pave   NaN      Reg   \n",
       "1456  1457          20       RL         85.0    13175   Pave   NaN      Reg   \n",
       "1457  1458          70       RL         66.0     9042   Pave   NaN      Reg   \n",
       "1458  1459          20       RL         68.0     9717   Pave   NaN      Reg   \n",
       "1459  1460          20       RL         75.0     9937   Pave   NaN      Reg   \n",
       "\n",
       "     LandContour Utilities  ... PoolArea PoolQC  Fence MiscFeature MiscVal  \\\n",
       "0            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "2            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "3            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "4            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "...          ...       ...  ...      ...    ...    ...         ...     ...   \n",
       "1455         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1456         Lvl    AllPub  ...        0    NaN  MnPrv         NaN       0   \n",
       "1457         Lvl    AllPub  ...        0    NaN  GdPrv        Shed    2500   \n",
       "1458         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1459         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "\n",
       "     MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0         2   2008        WD         Normal     208500  \n",
       "1         5   2007        WD         Normal     181500  \n",
       "2         9   2008        WD         Normal     223500  \n",
       "3         2   2006        WD        Abnorml     140000  \n",
       "4        12   2008        WD         Normal     250000  \n",
       "...     ...    ...       ...            ...        ...  \n",
       "1455      8   2007        WD         Normal     175000  \n",
       "1456      2   2010        WD         Normal     210000  \n",
       "1457      5   2010        WD         Normal     266500  \n",
       "1458      4   2010        WD         Normal     142125  \n",
       "1459      6   2008        WD         Normal     147500  \n",
       "\n",
       "[1460 rows x 81 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Решаем задачу линейной регресии \n",
    "\"\"\"\n",
    "data = pd.read_csv('train_1.csv')\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PoolQC          1453\n",
       "MiscFeature     1406\n",
       "Alley           1369\n",
       "Fence           1179\n",
       "FireplaceQu      690\n",
       "LotFrontage      259\n",
       "GarageYrBlt       81\n",
       "GarageCond        81\n",
       "GarageType        81\n",
       "GarageFinish      81\n",
       "GarageQual        81\n",
       "BsmtFinType2      38\n",
       "BsmtExposure      38\n",
       "BsmtQual          37\n",
       "BsmtCond          37\n",
       "BsmtFinType1      37\n",
       "MasVnrArea         8\n",
       "MasVnrType         8\n",
       "Electrical         1\n",
       "Id                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data.info()\n",
    "# Считаем кол-во пропусков в каждой колонке:\n",
    "nan_values = data.isnull().sum().sort_values(ascending=False) \n",
    "nan_values.head(20)\n",
    "\n",
    "# способ удалить пустые столбцы, где записей NaN, например, больше 100 \n",
    "# v = data.notna().sum().ge(100) # порог >=100 (.ge() означает больше или равно; .lt() - меньше; .gt() - больше; .eq() - равно)\n",
    "# либо\n",
    "# v = data.isna().sum().le(100) # в переменной - сумма пустых значений столбца больше или равно 100\n",
    "# deleted__na_columns = data.drop(v.index[v], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Id  MSSubClass MSZoning  LotArea Street LotShape LandContour  \\\n",
      "0        1          60       RL     8450   Pave      Reg         Lvl   \n",
      "1        2          20       RL     9600   Pave      Reg         Lvl   \n",
      "2        3          60       RL    11250   Pave      IR1         Lvl   \n",
      "3        4          70       RL     9550   Pave      IR1         Lvl   \n",
      "4        5          60       RL    14260   Pave      IR1         Lvl   \n",
      "...    ...         ...      ...      ...    ...      ...         ...   \n",
      "1455  1456          60       RL     7917   Pave      Reg         Lvl   \n",
      "1456  1457          20       RL    13175   Pave      Reg         Lvl   \n",
      "1457  1458          70       RL     9042   Pave      Reg         Lvl   \n",
      "1458  1459          20       RL     9717   Pave      Reg         Lvl   \n",
      "1459  1460          20       RL     9937   Pave      Reg         Lvl   \n",
      "\n",
      "     Utilities LotConfig LandSlope  ... EnclosedPorch 3SsnPorch ScreenPorch  \\\n",
      "0       AllPub    Inside       Gtl  ...             0         0           0   \n",
      "1       AllPub       FR2       Gtl  ...             0         0           0   \n",
      "2       AllPub    Inside       Gtl  ...             0         0           0   \n",
      "3       AllPub    Corner       Gtl  ...           272         0           0   \n",
      "4       AllPub       FR2       Gtl  ...             0         0           0   \n",
      "...        ...       ...       ...  ...           ...       ...         ...   \n",
      "1455    AllPub    Inside       Gtl  ...             0         0           0   \n",
      "1456    AllPub    Inside       Gtl  ...             0         0           0   \n",
      "1457    AllPub    Inside       Gtl  ...             0         0           0   \n",
      "1458    AllPub    Inside       Gtl  ...           112         0           0   \n",
      "1459    AllPub    Inside       Gtl  ...             0         0           0   \n",
      "\n",
      "     PoolArea MiscVal  MoSold  YrSold  SaleType  SaleCondition SalePrice  \n",
      "0           0       0       2    2008        WD         Normal    208500  \n",
      "1           0       0       5    2007        WD         Normal    181500  \n",
      "2           0       0       9    2008        WD         Normal    223500  \n",
      "3           0       0       2    2006        WD        Abnorml    140000  \n",
      "4           0       0      12    2008        WD         Normal    250000  \n",
      "...       ...     ...     ...     ...       ...            ...       ...  \n",
      "1455        0       0       8    2007        WD         Normal    175000  \n",
      "1456        0       0       2    2010        WD         Normal    210000  \n",
      "1457        0    2500       5    2010        WD         Normal    266500  \n",
      "1458        0       0       4    2010        WD         Normal    142125  \n",
      "1459        0       0       6    2008        WD         Normal    147500  \n",
      "\n",
      "[1460 rows x 62 columns]\n"
     ]
    }
   ],
   "source": [
    "# удалим пустые строки с записью NaN\n",
    "# data_filtred = data.dropna()\n",
    "data_ = pd.DataFrame.copy(data.drop(nan_values[nan_values>0].index,axis=1)) # axis='columns'== axis=1, а axis='index'== axis=0\n",
    "print(data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Находим категориальные признаки\n",
    "cat_feat = list(data_.dtypes[data_.dtypes == object].index) # index - берем построчно "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отфильтруем непрерывные признаки\n",
    "num_feat = [f for f in data_ if f not in (cat_feat + ['Id', 'SalePrice'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSZoning          5\n",
      "Street            2\n",
      "LotShape          4\n",
      "LandContour       4\n",
      "Utilities         2\n",
      "LotConfig         5\n",
      "LandSlope         3\n",
      "Neighborhood     25\n",
      "Condition1        9\n",
      "Condition2        8\n",
      "BldgType          5\n",
      "HouseStyle        8\n",
      "RoofStyle         6\n",
      "RoofMatl          8\n",
      "Exterior1st      15\n",
      "Exterior2nd      16\n",
      "ExterQual         4\n",
      "ExterCond         5\n",
      "Foundation        6\n",
      "Heating           6\n",
      "HeatingQC         5\n",
      "CentralAir        2\n",
      "KitchenQual       4\n",
      "Functional        7\n",
      "PavedDrive        3\n",
      "SaleType          9\n",
      "SaleCondition     6\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Смотрим, сколько у нас значений по каждому категориальному признаку\n",
    "cat_nunique = data_[cat_feat].nunique()                  # nunique() возвращает количество уникальных объектов (здесь-столбцов)\n",
    "print(cat_nunique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Сравним количество категориальных столбцов с количеством этих столбцов, содержащих уникальные значения \n",
    "print(len(cat_nunique)==len(cat_feat))\n",
    "# print(len(cat_feat))\n",
    "\n",
    "# # А можно было применить стратегию замены пустых значений столбцов на медианное значение признаков:\n",
    "# num_feat_median = data.loc[data[data[num_feat].isna()].index, num_feat] = data[num_feat].median() \n",
    "# num_feat_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединим 2 вида признаков в одной переменной, чтобы определить ее в Х \n",
    "\n",
    "# df = data.reset_index()\n",
    "\n",
    "factor_feat = pd.concat((data_[num_feat], data_[cat_feat]), axis=1) # объединяем столбцы фрейма data - категориальные\n",
    "                                                                  # признаки и непрерывные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задаем переменные Х и y\n",
    "X = data_[factor_feat.columns]  \n",
    "# X = data[data.columns[data.columns!='SalePrice']]   \n",
    "y = data['SalePrice'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбиваем на train/test\n",
    "D_train, D_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(X.isna().sum().sum())  # проверяем количество ячеек с пустыми значениями в переменной Х "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем дамми-переменные для категорий\n",
    "dummy_train = pd.get_dummies(D_train[cat_feat], columns=cat_feat)  # преобразовываем категориальную переменную \n",
    "dummy_test = pd.get_dummies(D_test[cat_feat], columns=cat_feat)    # в фиктивные / индикаторные переменные\n",
    "\n",
    "dummy_cols = list(set(dummy_train) & set(dummy_test))           # список уникальных (set) колонок категориальных признаков\n",
    "\n",
    "dummy_train_q = dummy_train[dummy_cols]                         # обучающийся сет категориальных признаков \n",
    "dummy_test_q = dummy_test[dummy_cols]                           # тестовый сет категориальных признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединяем 2 вида признаков в Х для обучающей и тестовой выборок \n",
    "X_train = pd.concat([D_train[num_feat],\n",
    "                     dummy_train_q], axis=1)\n",
    "\n",
    "X_test = pd.concat([D_test[num_feat],\n",
    "                     dummy_test_q], axis=1)\n",
    "\n",
    "# indices_to_keep_X_train = ~X_train.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "# indices_to_keep_X_test = ~X_test.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "# X_train = X_train[indices_to_keep_X_train].astype(np.float64)\n",
    "# X_test = X_test[indices_to_keep_X_test].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8724659227902871"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# определим модель линейной регресиии и обучим ее \n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train) \n",
    "\n",
    "# Получим прогнозные значения\n",
    "y_pred = model.predict(X_test)\n",
    "# Проверим качество модели \n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n2. Строим случайный лес и выводим важность признаков \\n'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "2. Строим случайный лес и выводим важность признаков \n",
    "\"\"\"\n",
    "# Разбиваем на train/test\n",
    "# splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "# for train_index, test_index in splitter.split(X, y):\n",
    "#     d_train = X.iloc[train_index]\n",
    "#     d_test = X.iloc[test_index]\n",
    "    \n",
    "#     y_train = data_['SalePrice'].iloc[train_index]\n",
    "#     y_test = data_['SalePrice'].iloc[test_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбиваем на train/test\n",
    "Df_train, Df_test, yf_train, yf_test = train_test_split(X, y, test_size=0.2, random_state=41)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем дамми-переменные для категорий\n",
    "dum_train = pd.get_dummies(Df_train[cat_feat], columns=cat_feat)  # преобразовываем категориальную переменную \n",
    "dum_test = pd.get_dummies(Df_test[cat_feat], columns=cat_feat)    # в фиктивные / индикаторные переменные\n",
    "\n",
    "dummy_col = list(set(dum_train) & set(dum_test))           # список уникальных (set) колонок категориальных признаков\n",
    "\n",
    "dum_train_q = dum_train[dummy_col]                         # обучающийся сет категориальных признаков \n",
    "dum_test_q = dum_test[dummy_col]                           # тестовый сет категориальных признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединяем 2 вида признаков в Х для обучающей и тестовой выборок \n",
    "Xf_train = pd.concat([Df_train[num_feat],\n",
    "                     dum_train_q], axis=1)\n",
    "\n",
    "Xf_test = pd.concat([Df_test[num_feat],\n",
    "                     dum_test_q], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=15, max_features='auto', max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=20,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
       "                      random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Обучаем решающее дерево\n",
    "# Немного ограничим глубину и минимальное кол-во объектов в листе для уменьшения переобучения\n",
    "\n",
    "\n",
    "clf_tree = RandomForestRegressor(max_depth=15, min_samples_leaf=20)\n",
    "clf_tree.fit(Xf_train, yf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вес наиболее важного признака:  0.7043102188888197\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.16958872e-03, 1.75321698e-02, 7.04310219e-01, 1.24912013e-03,\n",
       "       6.22254103e-03, 4.35189013e-03, 2.89833274e-02, 0.00000000e+00,\n",
       "       6.17491712e-04, 5.42745513e-02, 2.21381752e-02, 2.32418955e-03,\n",
       "       0.00000000e+00, 7.72035621e-02, 3.53351085e-04, 0.00000000e+00,\n",
       "       4.73882119e-03, 6.57452922e-04, 8.54064385e-05, 2.82922950e-04,\n",
       "       5.77072558e-03, 4.41007766e-03, 2.29209181e-02, 2.46930873e-02,\n",
       "       1.11835918e-03, 7.22493744e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 7.48210466e-05,\n",
       "       7.47967882e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 7.76340034e-04, 1.89287014e-05, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.85460963e-05,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.32490312e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.13996273e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.76491094e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 8.22284946e-06, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.91475238e-05, 0.00000000e+00,\n",
       "       2.31738152e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 2.11135739e-03, 0.00000000e+00, 4.66379145e-05,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.52971086e-03, 2.31719815e-05,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.69548223e-05, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       7.52949660e-05, 6.24319409e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.09040686e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.24226690e-05,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 9.42068621e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.08138141e-05, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 2.89938991e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 6.54959942e-05, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 6.22050429e-05, 0.00000000e+00,\n",
       "       9.78212623e-05, 1.56034846e-05])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Вес наиболее важного признака: ', max(clf_tree.feature_importances_))\n",
    "clf_tree.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2     0.704310\n",
       "13    0.077204\n",
       "9     0.054275\n",
       "6     0.028983\n",
       "23    0.024693\n",
       "        ...   \n",
       "87    0.000000\n",
       "86    0.000000\n",
       "85    0.000000\n",
       "84    0.000000\n",
       "89    0.000000\n",
       "Length: 178, dtype: float64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp = pd.Series(clf_tree.feature_importances_)\n",
    "imp.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8611800210994954"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверим качество модели (низкое)\n",
    "clf_tree.score(Xf_test, yf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. Обучить стекинг как минимум 3х моделей, используя хотя бы 1 линейную модель и 1 нелинейную\n",
    "\"\"\"\n",
    "\n",
    "# Вначале подготовим признаки для обучения \n",
    "# Разбиваем на train/test\n",
    "\n",
    "X = data[data.columns[data.columns!='SalePrice']]   \n",
    "y = data['SalePrice']\n",
    "\n",
    "Ds_train, Ds_test, ys_train, ys_test = train_test_split(X, y, test_size=0.3, random_state=39)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем дамми-переменные для категорий\n",
    "dummy_s_train = pd.get_dummies(Ds_train[cat_feat], columns=cat_feat)  # преобразовываем категориальную переменную \n",
    "dummy_s_test = pd.get_dummies(Ds_test[cat_feat], columns=cat_feat)    # в фиктивные / индикаторные переменные\n",
    "\n",
    "dummy_s_cols = list(set(dummy_s_train) & set(dummy_s_test))           # список уникальных (set) колонок категориальных признаков\n",
    "\n",
    "dummy_s_train_q = dummy_s_train[dummy_s_cols]                         # обучающийся сет категориальных признаков \n",
    "dummy_s_test_q = dummy_s_test[dummy_s_cols]                           # тестовый сет категориальных признаков]                           # тестовый сет категориальных признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для обучаемого сета находим медианное значение\n",
    "train_median = Ds_train[num_feat].median()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем переменную Х через конкатенацию признаков с обработанными значениями NaN\n",
    "Xs_train = pd.concat([Ds_train[num_feat].fillna(train_median),      # обрабатываем пустые значения, если таковые имеются\n",
    "                     Ds_train[num_feat + cat_feat].isnull().astype(np.int8).add_suffix('_NaN'),\n",
    "                     dummy_s_train_q], axis=1)\n",
    "\n",
    "Xs_test = pd.concat([Ds_test[num_feat].fillna(train_median),\n",
    "                     Ds_test[num_feat + cat_feat].isnull().astype(np.int8).add_suffix('_NaN'),\n",
    "                     dummy_s_test_q], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартиззируем выборку численных признаков путем удаления среднего и масштабирования до единичной дисперсии\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(Xs_train[num_feat])\n",
    "\n",
    "Xs_train[num_feat] = scaler.transform(Xs_train[num_feat])\n",
    "Xs_test[num_feat] = scaler.transform(Xs_test[num_feat])\n",
    "\n",
    "# Xs_train.shape, ys_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingRegressor(cv=10,\n",
       "                  estimators=[('lr',\n",
       "                               LinearRegression(copy_X=True, fit_intercept=True,\n",
       "                                                n_jobs=None, normalize=False)),\n",
       "                              ('rfr',\n",
       "                               RandomForestRegressor(bootstrap=True,\n",
       "                                                     ccp_alpha=0.0,\n",
       "                                                     criterion='mse',\n",
       "                                                     max_depth=None,\n",
       "                                                     max_features='auto',\n",
       "                                                     max_leaf_nodes=None,\n",
       "                                                     max_samples=None,\n",
       "                                                     min_impurity_decrease=0.0,\n",
       "                                                     min_impurity_split=None,\n",
       "                                                     min_samples_leaf=1,\n",
       "                                                     min_samples_spl...\n",
       "                                                     n_jobs=None,\n",
       "                                                     oob_score=False,\n",
       "                                                     random_state=44, verbose=0,\n",
       "                                                     warm_start=False)),\n",
       "                              ('knr',\n",
       "                               KNeighborsRegressor(algorithm='auto',\n",
       "                                                   leaf_size=30,\n",
       "                                                   metric='minkowski',\n",
       "                                                   metric_params=None,\n",
       "                                                   n_jobs=None, n_neighbors=5,\n",
       "                                                   p=2, weights='uniform'))],\n",
       "                  final_estimator=LinearRegression(copy_X=True,\n",
       "                                                   fit_intercept=True,\n",
       "                                                   n_jobs=None,\n",
       "                                                   normalize=False),\n",
       "                  n_jobs=None, passthrough=False, verbose=0)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Займемся непосредственым обученем стекинга \n",
    "# Финальное решение в стеке обучающихся моделей принимает логистическая регрессия  \n",
    "\n",
    "regr = StackingRegressor(\n",
    "    [\n",
    "        ('lr', LinearRegression()),\n",
    "        ('rfr', RandomForestRegressor(random_state=44)),\n",
    "        ('knr', KNeighborsRegressor())\n",
    "    ],\n",
    "LinearRegression(), cv=10)\n",
    "regr.fit(Xs_train, ys_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8328012100438943"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.score(Xs_test, ys_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# таким образом, видим, что наиболее точные предсказания дал метод обычной линейной регрессии\n",
    "# собственно, качество моделей высокое, поэтому можно остановиться. Либо \n",
    "# сформировать сетку гиперпараметров и выбрать наилучшие, воспользовавшись ими,\n",
    "# дабы улучшить качество прогнозирования "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
